---
title: "DS Salary Analysis"
author: "Caro Buck"
date: "2/22/2022"
output: html_document
---

Notes on setup/python installation [here](https://rstudio.github.io/reticulate/articles/python_packages.html).
Using R and Python together for good practice on integrating the two, even though I could've done this data pull + analysis using just one of the tools. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
path_to_python <- "/Users/caroline.buck/Library/r-miniconda/envs/r-reticulate/bin/python"
use_python(path_to_python)
py_config()

# Note: seems to work only with one package at a time
py_install('pandas')#,'requests')
py_install('requests')
pandas <- import('pandas')
requests <- import('requests')

```


### Get Levels.fyi Data

Following python tutorial found [here](https://towardsdatascience.com/a-beginners-guide-to-grabbing-and-analyzing-salary-data-in-python-e8c60eab186e) for getting the data.



```{python}
import pandas as pd
import requests

data = requests.get('https://www.levels.fyi/js/salaryData.json').json()
levels_raw = pd.DataFrame(data)
```


### Clean up data using R techinques
Looking at [site](https://www.levels.fyi/comp.html?track=Data%20Scientist&region=819,506&yoestart=3&yoeend=5), seems like there are 281 rows, but when I pull data I'm getting 160. Good enough for this analysis. 

```{r}
library(tidyverse)
# get the python object from prev cell into R environment
levels_raw_r <- py$levels_raw
levels_raw_r %>%
  select(-rowNumber) %>%
  filter(title == 'Data Scientist') %>%
  filter(dmaid %in% c('506','819')) %>%
  filter(yearsofexperience %in% c('3','4','5')) -> levels_clean
```

### Now time to summarise and visualize results!

Varies a lot, let's look at impact of level and/or company
```{r}
levels_clean %>%
  select(totalyearlycompensation) %>%
  mutate(totalyearlycompensation = as.numeric(totalyearlycompensation)) %>%
  # remove obvious outliers/data errors
  filter(totalyearlycompensation > 0 & totalyearlycompensation < 800) %>%
  summary(totalyearlycompensation) 
```

There are 41 companies listed...names need some clean up (Amazon listed a few times)
```{r}
levels_clean %>%
  select(totalyearlycompensation,company) %>%
  mutate(totalyearlycompensation = as.numeric(totalyearlycompensation)) %>%
  # remove obvious outliers/data errors
  filter(totalyearlycompensation > 0 & totalyearlycompensation < 800) %>%
  group_by(company) %>%
  summarise(min=min(totalyearlycompensation),
            median=median(totalyearlycompensation),
            mean=mean(totalyearlycompensation),
            max=max(totalyearlycompensation))
  
```

There are also a lot (44) of levels listed, some of which I have no idea what they mean. 
```{r}
levels_clean %>%
  select(totalyearlycompensation,level) %>%
  mutate(totalyearlycompensation = as.numeric(totalyearlycompensation)) %>%
  # remove obvious outliers/data errors
  filter(totalyearlycompensation > 0 & totalyearlycompensation < 800) %>%
  group_by(level) %>%
  summarise(min=min(totalyearlycompensation),
            median=median(totalyearlycompensation),
            mean=mean(totalyearlycompensation),
            max=max(totalyearlycompensation))
  
```

```{r}
levels_clean %>%
  select(totalyearlycompensation,level,company,location,dmaid) %>%
  mutate(totalyearlycompensation = as.numeric(totalyearlycompensation),
         dma_label = case_when(dmaid == 506 ~ 'Boston',TRUE ~ 'Seattle')) %>%
  # remove obvious outliers/data errors
  filter(totalyearlycompensation > 0 & totalyearlycompensation < 800) %>%
  ggplot(aes(x=totalyearlycompensation,y=1)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_jitter(width = 0,height = .1) +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  facet_grid(dma_label~.)
  
```

# todo:
filter to level that seems most appropriate for me, and get summary stats. 
